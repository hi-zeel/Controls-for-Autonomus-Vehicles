{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Predictive Control (MPC) - Basics\n",
    "### Notebook 1 of 3: Fundamentals\n",
    "\n",
    "---\n",
    "This notebook introduces **Model Predictive Control (MPC)** step by step, starting from **Linear Quadratic Regulation (LQR)**.\n",
    "\n",
    "We will cover:\n",
    "- Motivation: Why MPC?\n",
    "- PID vs LQR vs MPC\n",
    "- LQR derivation (theory + Riccati solution)\n",
    "- Example: Double integrator system under LQR\n",
    "- Transition from LQR to MPC\n",
    "- Brunton-style diagrams (prediction horizon, block diagrams)\n",
    "- Exercises for tuning $Q$ and $R$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation\n",
    "\n",
    "- **PID**: reactive, hand-tuned, no explicit model.\n",
    "- **LQR**: optimal control for linear systems, fixed feedback gain.\n",
    "- **MPC**: uses the model *online*, optimizes at each step, handles constraints.\n",
    "\n",
    "### Key Question\n",
    "Why do we need MPC when LQR already gives an optimal solution?\n",
    "- Because LQR cannot enforce constraints.\n",
    "- MPC can predict the future and act conservatively if constraints might be violated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LQR Theory\n",
    "\n",
    "We consider the linear system:\n",
    "\n",
    "$$\n",
    "x_{k+1} = A x_k + B u_k.\n",
    "$$\n",
    "\n",
    "We minimize the quadratic cost:\n",
    "\n",
    "$$\n",
    "J = \\sum_{k=0}^\\infty (x_k^T Q x_k + u_k^T R u_k).\n",
    "$$\n",
    "\n",
    "Solution: feedback law\n",
    "\n",
    "$$\n",
    "u_k = -Kx_k, \\quad K = (R + B^T P B)^{-1} B^T P A.\n",
    "$$\n",
    "\n",
    "where $P$ solves the **Discrete Algebraic Riccati Equation (DARE)**:\n",
    "\n",
    "$$\n",
    "P = A^T P A - A^T P B (R + B^T P B)^{-1} B^T P A + Q.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LQR Gain K = [[0.42208244 1.24392885]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "\n",
    "# Double integrator system\n",
    "A = np.array([[1, 1],\n",
    "              [0, 1]])\n",
    "B = np.array([[0],\n",
    "              [1]])\n",
    "Q = np.eye(2)\n",
    "R = np.array([[1]])\n",
    "\n",
    "# Solve Discrete Algebraic Riccati Equation\n",
    "P = la.solve_discrete_are(A, B, Q, R)\n",
    "K = la.inv(B.T @ P @ B + R) @ (B.T @ P @ A)\n",
    "print(\"LQR Gain K =\", K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "- Try changing $Q$ and $R$.\n",
    "- Example: $Q=10I$, $R=1$ vs. $Q=I$, $R=10$.\n",
    "- Which case produces more aggressive control?\n",
    "\n",
    "### Exercise 1 - Solution\n",
    "Case 1: $Q = 10I, \\; R = 1$  \n",
    "- Large $Q$: strong penalty on state error.  \n",
    "- Small $R$: weak penalty on control effort.  \n",
    "- $\\Rightarrow$ **Aggressive control**: large inputs, fast convergence.  \n",
    "\n",
    "Case 2: $Q = I, \\; R = 10$  \n",
    "- Small $Q$: mild penalty on state error.  \n",
    "- Large $R$: strong penalty on control effort.  \n",
    "- $\\Rightarrow$ **Conservative control**: smoother inputs, slower convergence.  \n",
    "\n",
    "**Conclusion:**  \n",
    "- Large $Q$, small $R$ $\\;\\Rightarrow\\;$ Aggressive control  \n",
    "- Small $Q$, large $R$ $\\;\\Rightarrow\\;$ Conservative control  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LQR Simulation (Double Integrator)\n",
    "\n",
    "We simulate the closed-loop system with $u = -Kx$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c1cb0f76434377818c70064ba6f952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='Q_pos', max=20.0, min=0.1, step=0.5), FloatSlider(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "def lqr_demo(Q_pos=1.0, Q_vel=1.0, R_val=1.0):\n",
    "    Q = np.diag([Q_pos, Q_vel])\n",
    "    R = np.array([[R_val]])\n",
    "    P = la.solve_discrete_are(A, B, Q, R)\n",
    "    K = la.inv(B.T @ P @ B + R) @ (B.T @ P @ A)\n",
    "    \n",
    "    x = np.array([[5],[0]])\n",
    "    steps = 30\n",
    "    x_hist = []\n",
    "    for t in range(steps):\n",
    "        u = -K @ x\n",
    "        x = A @ x + B @ u\n",
    "        x_hist.append(x.flatten())\n",
    "    x_hist = np.array(x_hist)\n",
    "    \n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(x_hist[:,0], label=\"Position\")\n",
    "    plt.plot(x_hist[:,1], label=\"Velocity\")\n",
    "    plt.title(f\"LQR with Q=[{Q_pos}, {Q_vel}], R={R_val}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "interact(lqr_demo, Q_pos=(0.1, 20, 0.5), Q_vel=(0.1, 20, 0.5), R_val=(0.1, 20, 0.5));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. From LQR to MPC\n",
    "\n",
    "- **LQR**: unconstrained, infinite horizon, fixed $K$.\n",
    "- **MPC**: constrained, finite horizon, solved repeatedly.\n",
    "\n",
    "### MPC Cost Function\n",
    "$$\n",
    "J = \\sum_{i=0}^{N-1} (x_{k+i}^T Q x_{k+i} + u_{k+i}^T R u_{k+i}) + x_{k+N}^T P x_{k+N}.\n",
    "$$\n",
    "\n",
    "Constraints:\n",
    "- $u_{min} \\le u \\le u_{max}$\n",
    "- $x_{min} \\le x \\le x_{max}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagram: Prediction Horizon\n",
    "N = 5\n",
    "x_vals = np.arange(N+1)\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(x_vals, np.zeros_like(x_vals), 'ko-', label='Predicted trajectory')\n",
    "for i in range(N):\n",
    "    plt.arrow(x_vals[i], 0, 1, 0, head_width=0.05, head_length=0.2, fc='blue', ec='blue')\n",
    "plt.axvline(x=0, color='red', linestyle='--', label='Current state')\n",
    "plt.axvline(x=N, color='green', linestyle='--', label='End of horizon')\n",
    "plt.title('Prediction Horizon in MPC')\n",
    "plt.xlabel('Future time steps')\n",
    "plt.yticks([])\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Block Diagram Comparison\n",
    "\n",
    "We compare LQR vs MPC architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "# LQR block diagram\n",
    "axes[0].set_title('LQR Architecture')\n",
    "axes[0].arrow(0.1,0.5,0.3,0,head_width=0.05, head_length=0.05, fc='k')\n",
    "axes[0].text(0.05,0.5,'State x')\n",
    "rect = mpatches.Rectangle((0.4,0.4),0.2,0.2,fill=False)\n",
    "axes[0].add_patch(rect)\n",
    "axes[0].text(0.5,0.5,'-K')\n",
    "axes[0].arrow(0.6,0.5,0.3,0,head_width=0.05, head_length=0.05, fc='k')\n",
    "axes[0].text(0.95,0.5,'u')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# MPC block diagram\n",
    "axes[1].set_title('MPC Architecture')\n",
    "axes[1].arrow(0.05,0.5,0.25,0,head_width=0.05, head_length=0.05, fc='k')\n",
    "axes[1].text(0.0,0.5,'State x')\n",
    "rect2 = mpatches.Rectangle((0.3,0.4),0.4,0.2,fill=False)\n",
    "axes[1].add_patch(rect2)\n",
    "axes[1].text(0.5,0.5,'Optimizer\\n(min cost)')\n",
    "axes[1].arrow(0.7,0.5,0.25,0,head_width=0.05, head_length=0.05, fc='k')\n",
    "axes[1].text(0.95,0.5,'u')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "- Explain in your own words: why is MPC more flexible than LQR?\n",
    "- Hint: Think about constraints and re-solving.\n",
    "\n",
    "### Exercise 2 - Solution\n",
    "- **LQR**: one-time optimization → fixed feedback gain, no constraints.  \n",
    "- **MPC**: re-solves optimization every step, adapts to states, and enforces constraints.  \n",
    "\n",
    "$\\Rightarrow$ MPC is more flexible than LQR.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
